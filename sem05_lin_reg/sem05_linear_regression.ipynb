{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение\n",
    "\n",
    "## Факультет математики НИУ ВШЭ\n",
    "\n",
    "### 2018-2019 учебный год\n",
    "\n",
    "Лектор: Илья Щуров\n",
    "\n",
    "Семинаристы: Евгения Ческидова, Евгений Ковалев\n",
    "\n",
    "Ассистенты: Константин Ваниев, Софья Дымченко"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Что сегодня узнаем\n",
    "\n",
    "1. Что такое градиентный спуск и как он работает\n",
    "2. Как обучить линейную регрессию с помощью градиентного спуска\n",
    "3. Что такое коэффициент детерминации\n",
    "4. Как линейная зависимость признаков влияет на построение линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.linalg import norm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия: напоминание\n",
    "\n",
    "В задаче линейной регрессии мы предполагаем, что ответ является линейной функцией от данных, то есть\n",
    "\n",
    "$$y = w_1 x_1 + \\ldots + w_m x_m = \\langle x, w\\rangle,$$\n",
    "\n",
    "* Здесь $\\langle\\cdot, \\cdot\\rangle$ — скалярное произведение.\n",
    "* $w_i$ — вес i-ого признака в модели линейной регрессии. \n",
    "    * $w=(w_1, \\ldots, w_m)$ — вектор весов признаков.\n",
    "* $x_i$ — значение i-ого признака во входном $x$ \n",
    "\n",
    "Распространенным способом обучения линейной регрессионной модели является метод наименьших квадратов. При использовании этого метода минимизируется *квадратичная функция потерь*:\n",
    "\n",
    "$$Q(w) = \\sum_{j=1}^n Q_j(w) = \\sum_{j=1}^n ( \\langle x^j, w\\rangle - y^j)^2$$\n",
    "\n",
    "Здесь $n$ — число элементов в обучающей выборке.\n",
    "\n",
    "На лекции мы нашли явную формулу для вектора весов, решающую минимизационную задачу:\n",
    "\n",
    "$$w = (X^T X)^{-1} X^T y,$$\n",
    "\n",
    "где $X$ — матрица объект-признак (по строкам объекты, по столбцам признаки), $y$ — вектор правильных ответов.\n",
    "\n",
    "Эта формула очень полезна для теоретического анализа, но имеет некоторые ограничения:\n",
    "\n",
    "1. В ней используется «дорогая» операция — обращение матрицы размером $d\\times d$, где $d$ — количество признаков. Она занимает $O(d^3)$ операций, и если $d$ большое, может быть довольно медленной.\n",
    "2. Эта формула выведена в предположении именно квадратичной функции потерь. Если мы захотим использовать другую формулу потерь, она не работает (и не всегда можно вывести такую явную формулу, которая бы работала). \n",
    "\n",
    "Вместо использования явной формулы можно предложить другой подход — минимизация $Q$ с помощью итеративных методов. Простейшим из них является метод градиентного спуска. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию $f(x, y)=x^2 + 10 y^2$. Её градиент равен $\\nabla f(x, y)=\\frac{\\partial f(x, y)}{\\partial(x, y)}=(2x, 20y)$. Он показывает направление наискорейшего роста функции, то есть отвечает на вопрос «куда нам идти, если мы находимся в точке $(x, y)$ и хотим увеличивать значение функции как можно быстрее». Чтобы уменьшать значение функции, нужно идти в противоположном направлении. В связи с этим, напрашивается такой алгоритм нахождения минимума функции $f$:\n",
    "\n",
    "1. Возьмём любую точку $(x_0, y_0)$. Посчитаем градиент в этой точке.\n",
    "\n",
    "2. Для каждого $i=1, \\ldots$, положим: $(x_{i+1}, y_{i+1})=(x_i, y_i) - \\eta \\nabla f(x_i, y_i)$, где $\\eta$ — какое-то число (небольшое).\n",
    "\n",
    "3. Будем продолжать вычислять очередную точку $(x_i, y_i)$ до тех пор, пока мы не окажемся достаточно близко к минимуму — например, до тех пор, пока градиент не слишком маленький."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(u):\n",
    "    return u[0] ** 2 + 10 * u[1] ** 2\n",
    "def Df(u):\n",
    "    return np.array([2*u[0], 20 * u[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_simple(f, Df, eta=0.01, steps=20000, initial_point=(-3, 3), \n",
    "                            precision=1e-10, xmin=-4, xmax=4, ymin=-3, \n",
    "                            ymax=3, allpoints=False):\n",
    "    u_prev = np.array(initial_point)\n",
    "\n",
    "\n",
    "    X = np.linspace(xmin, xmax, 100)\n",
    "    Y = np.linspace(ymin, ymax, 100)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.contour(X, Y, [[f(np.array([x, y])) for x in X] for y in Y], 100)\n",
    "    # нарисуем линии уровня функции $f$\n",
    "\n",
    "    points = []\n",
    "    for i in range(steps):\n",
    "        points.append(u_prev)\n",
    "        u_new = u_prev - eta * Df(u_prev)\n",
    "\n",
    "        if norm(u_new - u_prev) < precision:\n",
    "            break\n",
    "        # прекратить, если новая точка достаточно близка к старой\n",
    "\n",
    "        u_prev = u_new\n",
    "\n",
    "\n",
    "    plt.plot([p[0] for p in points], [p[1] for p in points], 'o-')\n",
    "    if allpoints:\n",
    "        return points\n",
    "    return points[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_simple(f=f, Df=Df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если взять слишком большую $\\eta$, сходимости может и не быть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_simple(f=f, Df=Df, eta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если $\\eta$ слишком маленькая, то мы можем не дойти до минимума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_simple(f=f, Df=Df, eta=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим функцию посложнее — возьмём так называемую [функцию Розенброка](https://ru.wikipedia.org/wiki/Функция_Розенброка)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosenbrock(u):\n",
    "    return ((1 - u[0]) ** 2\n",
    "            + 100 * (u[1] - u[0] ** 2) ** 2)\n",
    "def Drosenbrock(u):\n",
    "    return (np.array([-2 * (1 - u[0]) \n",
    "                      - 100 * 2 * (u[1] - u[0] ** 2) * 2 * u[0], \n",
    "                      100 * 2 * (u[1] - u[0] ** 2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_simple(f=rosenbrock, Df=Drosenbrock,\n",
    "                        initial_point=(-3, 3), \n",
    "                        eta=0.0001, ymax=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = gradient_descent_simple(f=rosenbrock, Df=Drosenbrock, \n",
    "                                 initial_point=(-3, 3), eta=0.001 / 1.2, \n",
    "                                 ymax=5, allpoints=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Градиентный спуск своими руками\n",
    "\n",
    "Предлагается найти градиент квадратичной функции потерь и реализовать градиентный спуск для модельной задачи.\n",
    "\n",
    "Пусть идеальная зависимость задается следующим образом: $y = kx + b$\n",
    "\n",
    "Пусть \n",
    "* k=2\n",
    "* b=3\n",
    "\n",
    "\n",
    "Добавим к данным немного шума из нормального распределения и постараемся восстановить параметры $w = (k, b)$\n",
    "\n",
    "Для этого нам необходимо знать градиент для квадратичной функции потерь.\n",
    "\n",
    "Давайте распишем градиент для одномерного случая. Вывод в общем виде можно посмотреть в конспекте лекции (когда Илья его напишет)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сгенерируем выборку:\n",
    "from numpy.random import normal\n",
    "\n",
    "n = 100\n",
    "\n",
    "# сгенерировали n примеров x из нормального распределения\n",
    "x = normal(size=n)\n",
    "\n",
    "# генерируем зависимость с добавлением шума\n",
    "y = 2 * x + 3 + normal(size=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим, что у нас за данные\n",
    "plt.plot(x, y, 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Добавление свободного члена**\n",
    "\n",
    "Поскольку наша регрессия имеет свободный член, к матрице $X$ нужно приписать столбец, состоящий из одних единиц. В данном случае мы использовали функцию `np.ones_like()`, генерирующую объект с такими же измерениями, как и его аргумент, заполненный единицами. В данном случае мы попросили сделать вектор такой же длины, как $x$, но заполненный единицами. `np.array` в данном случае создаёт матрицу из списка списков, считая, что она записана по строкам; чтобы её записать по столбцам, мы применили транспонирование."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([x, np.ones_like(x)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим теперь на первые 5 примеров\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем квадратичную функцию потерь и её градиент."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q(w):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQ(w):\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_descent_simple(f=Q, Df=DQ, xmin=1, xmax=3, ymin=1, ymax=4, \n",
    "                        initial_point=(1, 1), eta=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, получается результат, похожий на настоящие значения $k$ и $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия из коробки\n",
    "\n",
    "Воспользуемся самым простым способом построить регрессию, используя готовые модели из sklearn. Запуская регрессию из коробки, необходимо читать, что [внутри](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Загружаем датасет\n",
    "\n",
    "# Source: https://archive.ics.uci.edu/ml/machine-learning-databases/communities/\n",
    "# Redmond, M. A. and A. Baveja: A Data-Driven Software Tool for Enabling Cooperative \n",
    "# Information Sharing Among Police Departments. European Journal of Operational Research \n",
    "# 141 (2002) 660-678. \n",
    "\n",
    "x = pd.read_csv('https://raw.githubusercontent.com/ischurov/math-ml-hse-2018/master/sem05_lin_reg/communities.csv')\n",
    "\n",
    "#Выделяем вектор y в отдельную переменную \n",
    "y = x['ViolentCrimesPerPop']\n",
    "y = pd.DataFrame(y)\n",
    "x = x.drop(['ViolentCrimesPerPop'],1)\n",
    "colnames = x.columns\n",
    "\n",
    "# Заполняем все пропуски в данных средними значениями для каждого признака\n",
    "from sklearn import preprocessing\n",
    "mis_replacer = preprocessing.Imputer(strategy=\"mean\")\n",
    "x = pd.DataFrame(data=mis_replacer.fit_transform(x),columns=x.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также мы изначально разобьем наши данные пополам и пока что будем работать только с одной половиной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,y_tr = x.iloc[:int(x.shape[0]/2)], y.iloc[:int(x.shape[0]/2)]\n",
    "x_test,y_test = x.iloc[int(x.shape[0]/2)+1:], y.iloc[int(x.shape[0]/2)+1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Импортируем класс из sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "#непосредсвенно строим регрессию\n",
    "lr.fit(x_tr,y_tr)\n",
    "#Готово!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу замечаем, что по умолчанию в число регрессоров будет включена и константа. С помощью методов класса, из нашей построенной регрессии можно получить вектор весов $w$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = lr.coef_[0]\n",
    "w = pd.DataFrame({'Имя признака':colnames,'Вес признака':w},\n",
    "                 columns=['Имя признака','Вес признака'])\n",
    "w[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим прогноз $a$ на основе нашей модели:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = lr.predict(x_tr)\n",
    "a = pd.DataFrame(a, columns=['prediction'])\n",
    "# прогноз для первых нескольких объектов\n",
    "a.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $R^2$ — основная метрика качества регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда необходимо сравнить по качеству две разных линейных модели, то чаще всего для этого используют метрику $R^2$. Основное преимущество данной метрики: её значение может интерпретироваться. $R^2$ - это с некоторыми оговорками доля дисперсии, объяснённая моделью в общей дисперсии целевой переменной. Данный показатель принимает значения в диапазоне от 0 до 1. Чем он больше, тем лучше построенная модель предсказывает поведение целевой переменной.  \n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - a(x_i))^2}{\\sum_i (y_i - \\overline{y}_i)^2} \\approx 1 - \\frac{\\text{Var} (Y|X)}{\\text{Var} Y}$$\n",
    "\n",
    "Теперь посчитаем $R^2$ для построенной нами регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#первый способ\n",
    "print('Первый способ: %f' %lr.score(x_tr,y_tr))\n",
    "#второй способ\n",
    "import sklearn.metrics as metrics\n",
    "print('Второй способ: %f' %metrics.r2_score(y_tr,a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70% объяснённой дисперсии целевой переменной. Много это или мало — зависит от конкретной задачи и от наших субъективных ожиданий.  Если в структуре наших данных почти нет закономерностей и всё подвержено случайным событиям, то ни одна модель не сможет дать нам высокий $R^2$. В таком случае и 15% было бы неплохим результатом."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Валидация — проверка обобщающей способности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$, который приведён выше, посчитан на тех данных, на которых была построена регрессия. Но мы хотим, чтобы построенная модель успешно справлялась с прогнозами и на тех на данных, для которых ей неизвестно истинное значение целевой переменной, иначе практическая польза такой модели будет мала. Если модель способна выявить в данных какие-либо закономерности и обобщить их для данных, которые не использовались при её построении, то мы говорим, что она обладает обобщающей способностью. Проверку обобщающей способности мы назовём валидацией. В конечном счёте, нас интересует $R^2$, который модель покажет на новых данных.\n",
    "\n",
    "Вспомним об отложенной нами выборке и воспользуемся ей для проверки обобщающей способности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_test = lr.predict(x_test)\n",
    "print('R2 на тестовой выборке: %f' %metrics.r2_score(y_test,a_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подводные камни\n",
    "\n",
    "Oops... something went wrong...: $R^2$ оказался отрицательным. Как это произошло? Обратим внимание на дробь:\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum_i (y_i - a(x_i))^2}{\\sum_i (y_i - \\overline{y}_i)^2} \\approx 1 - \\frac{\\text{Var} (Y|X)}{\\text{Var} Y}$$\n",
    "\n",
    "В ней дисперсия целевой переменной при нашей модели сравнивается с обычной дисперсией. Обычную дисперсию можно представить как условную дисперсию целевой переменной в *\"базовой\"* модели, которая рисует горизонтальную прямую на уровне $\\bar{y}$. Выходит, что для регрессий, которые справляются с предсказанием хуже, чем базовая регрессия, зашитая прямо в формулу данной метрики, $R^2$ выдает отрицательный результат:\n",
    "![pictcha](https://raw.githubusercontent.com/FilatovArtm/ML_Eco-NES_2017/master/seminars/maintain/sem3picture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, мы, решив провести валидацию, сразу поняли, что на новой выборке наша модель хуже, чем отсутствие модели вообще (прогноз $a(.)=\\bar{y}$ не требует $x$ для построения). Поэтому обобщающая способность нашей модели равна нулю. К счастью, проблему можно решить. Посмотрим на вектор $w$ ещё раз, при этом отсортировав:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w.sort_values(by='Вес признака')[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И сразу замечаем, что у 1-ого регрессора в данном списке очень маленькое значение по сравнению с другими. А если обратим внимание на дно данного списка, то заметим брата-близнеца данного регрессора, имеющего очень большое значение и по модулю равное ему! В этот момент мы понимаем в чём дело. Похоже в нашей матрице \"объекты-признаки\" есть линейно-зависимые регрессоры. Посмотрим на корреляцию между ними: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(x_tr['PolicPerPop'],x_tr['LemasSwFTPerPop'])[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корреляция близка к единице. Следовательно, в наших данных два почти одинаковых регрессора.\n",
    "\n",
    "Что будет, если включить в модель линейно-зависимые признаки? Приведем пример. Пусть мы имеем дело с *истинной зависимостью*:\n",
    "\n",
    "$$y_i=7+2p_i+3q_i+4r_i,$$\n",
    "\n",
    "где истинный вектор весов имеет вид $w=(w_0,w_1,w_2,w_3)^T=(7,2,3,4)$. Пусть переменные $q_i$ и $r_i$ всегда одинаковы:\n",
    "\n",
    "$$q_i=r_i$$ \n",
    "\n",
    "Тогда существует бесконечное число комбинаций $w_2$ и $w_3$, которые нам всё равно бы подошли. Например, возьмём комбинацию $w*=(7,2,-2000,2007)$:\n",
    "\n",
    "$$7+2p_i-2000q_i+2007r_i\\\\7+2p_i-2000q_i+2007q_i\\\\7+2p_i+7q_i\\\\7+2p_i+3q_i+4q_i\\\\7+2p_i+3q_i+4r_i=y_i$$\n",
    "\n",
    "Поэтому нам всё равно, какой вектор из бесконечного числа комбинаций взять. И обычно компьютер выбирает тот, в котором веса очень большие. А это означает, что если в новой выборке найдутся объекты, у которых данные регрессоры будут отличаться чуть больше, чем в обучающей выборке, то это будет катастрофически влиять на значение $y$ у таких объектов, далеко отбрасывая их даже от $\\bar{y}$. В эконометрике данное явление называется мультиколлинеарностью, и мы рассмотрели его лишь с одной стороны.  \n",
    "\n",
    "__Решение проблемы:__ Мы посмотрели описания признаков \"PolicPerPop\" и \"LemasSwFTPerPop\" и обнаружили, что они означают почти одно и то же (число полицейских на душу населения), поэтому мы просто удалили один из признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.drop(['LemasSwFTPerPop'],1)\n",
    "colnames  = x.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr,y_tr = x.iloc[:int(x.shape[0]/2)],y.iloc[:int(x.shape[0]/2)]\n",
    "x_test,y_test = x.iloc[int(x.shape[0]/2)+1:],y.iloc[int(x.shape[0]/2)+1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим регрессию вновь и обнаружим, что теперь она приобрела обобщающую способность:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(x_tr,y_tr)\n",
    "w = lr.coef_[0]\n",
    "w = pd.DataFrame({'Имя признака':colnames,'Вес признака':w},columns = ['Имя признака','Вес признака'])\n",
    "\n",
    "a = lr.predict(x_tr)\n",
    "a_test = lr.predict(x_test)\n",
    "print('R2 на обучающей выборке: %f' %metrics.r2_score(y_tr,a))\n",
    "print('R2 на тестовой выборке: %f' %metrics.r2_score(y_test,a_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
