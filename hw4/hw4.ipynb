{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение на факультете математики\n",
    "## НИУ ВШЭ, 2018-19 учебный год\n",
    "### Домашнее задание №4\n",
    "[Страница курса](http://wiki.cs.hse.ru/%D0%9C%D0%B0%D1%88%D0%B8%D0%BD%D0%BD%D0%BE%D0%B5_%D0%BE%D0%B1%D1%83%D1%87%D0%B5%D0%BD%D0%B8%D0%B5_%D0%BD%D0%B0_%D0%BC%D0%B0%D1%82%D1%84%D0%B0%D0%BA%D0%B5_2018/2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание выполнил(а): _(впишите свои фамилию и имя)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Внимание!__ Домашнее задание выполняется самостоятельно. При попытке сдать хотя бы частично списанный текст, или текст, полученный в результате совместного решения задач, вся работа будет оценена на 0 баллов. Мы также уведомим администрацию факультета и попросим применить дисциплинарное взыскание (предупреждение, выговор, отчисление) ко всем вовлеченным студентам.\n",
    "\n",
    "## Правила ##\n",
    "\n",
    "При выполнении этого ДЗ вы можете использовать компьютер, но не можете пользоваться специализированными библиотеками для машинного обучения (`sklearn`, `pytorch`, `tensorflow` и т.д.). При необходимости можно использовать `numpy`, `sympy`, `pandas`. Хотя в целом эту домашку можно сделать и просто с ручкой и бумажкой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 1 (10 баллов)\n",
    "У рассеянного Александра из прошлого ДЗ есть вектор ответов $(y_1, \\ldots, y_n)$ для задачи классификации, каждый $y_i\\in \\{0, 1\\}$, $n$ нечётное число, всего среди $y_i$ есть $m$ единиц и $(n-m)$ нулей. Как и в прошлый раз, Александр потерял матрицу признаков, поэтому вынужден использовать алгоритм, обучающийся только по ответам. Он хочет, чтобы алгоритм предсказывал вероятность $p$ получения единицы, и думает, какую функцию потерь ему выбрать из двух возможных:\n",
    "\n",
    "1. Log-loss: $$L_{LL}(y, p)=\\begin{cases}\n",
    "-\\log p,&\\text{ if }y=1;\\\\\n",
    "-\\log(1-p),&\\text{ if }y=0.\n",
    "\\end{cases}$$\n",
    "\n",
    "2. Абсолютное отклонение: $$L_{AD}(y, p)=|y-p|.$$\n",
    "\n",
    "Для нахождения оптимального $p$ Александр решает задачу минимизиации эмпирического риска:\n",
    "\n",
    "$$\\sum_{i=1}^n L(y_i, p) \\to \\min_{p},$$\n",
    "где $L$ — это либо $L_{LL}$, либо $L_{AD}$.\n",
    "\n",
    "Какое $p$ получится у Александра для каждой из данных функций потерь? Какую из функций потерь следует использовать, если Александр хочет, чтобы $p$ была состоятельной оценкой для вероятности получения единицы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 2 \n",
    "Кларисса решает задачу двухлассовой классификации (классы обозначаются $+1$ и $-1$) с помощью алгоритма машинного обучения, который для $i$-го объекта выдаёт степень уверенности $s_i$ алгоритма в том, что этот объект принадлежит к классу $+1$ (например, это может быть оценка вероятности, данная логистической регрессией). Кларисса выбирает пороговое значение $t$, после чего все объекты, для которых $s_i>t$, относит к положительному классу, а остальные — к отрицательному. Иными словами, окончательное предсказание классификатора имеет вид:\n",
    "$$\\hat y_i = [s_i>t] - [s_i\\le t].$$\n",
    "\n",
    "В таблице для каждого элемента обучающей выборки даны значения $s_i$ и их истинные классы. Построить ROC-кривую для данного алгоритма.\n",
    "\n",
    "$$\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "s_i & y_i \\\\\n",
    "\\hline\n",
    "0.6 & +1\\\\\n",
    "0.5 & -1\\\\\n",
    "0.1 & -1\\\\\n",
    "0.2 & -1\\\\\n",
    "0.4 & +1\\\\\n",
    "0.7 & +1\\\\\n",
    "\\hline\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 3\n",
    "$$\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "i & x_i & y_i \\\\\n",
    "\\hline\n",
    "1 & 1 & good \\\\\n",
    "2 & 2 & good \\\\\n",
    "3 & 3 & bad \\\\\n",
    "4 & 10 & good \\\\\n",
    "\\hline\n",
    "\\end{array}$$\n",
    "Профессор Стамп обучает случайный лес для решения задачи предсказания погоды на следующий день в завимости от того, сколько раз кукушка прокуковала перед закатом. Погода бывает *хорошей* и *плохой*, в подробости он не вдаётся. Его обучающая выборка приведена в таблице. Лес состоит из трёх *решающих пней*, то есть решающих деревьев с единственной нетерминальной вершиной. В результате процедуры бэггинга первому пню досталась выборка, полученная из исходной выбором наблюдений 1, 2, 3 и снова 3, второму — наблюдения 2, снова 2, 3 и 4, а третьему — наблюдения 1, 3, снова 3 и опять 3. Каждый пень делит выборку на две части в зависимости от результата сравнения $x_i$ с порогом $t$. Значение порога выбирается таким образом, чтобы минимизировать функцию\n",
    "\n",
    "$$Q(L, R)=|L|\\cdot I_G(L)+|R|\\cdot I_G(R),$$\n",
    "где $I_G(M)$ — значение [Gini impurity function](https://en.wikipedia.org/wiki/Decision_tree_learning#Gini_impurity) на соответствующем множестве, $L$ и $R$ — части, на которые делится выборка, $|R|$ и $|L|$ — количество элементов в соответствующем множестве. Значения $t$ выбираются ровно посередине между ближайшими значениями $x_i$ в выборке. Если есть несколько одинаково оптимальных с точки зрения функции $Q$ значений $t$, выбирается меньшее из них. В качестве предсказания каждый из пней выдаёт наиболее часто встречающийся класс в соответствующей части выборки; если объектов каждого класса поровну, то предсказывается good (пни оптимистичны). Предсказание всего леса определяется в результате голосования каждого из пней.\n",
    "\n",
    "1. Найти пороговые значения для каждого пня.\n",
    "2. Какой класс предскажет лес для наблюдения с x=7?\n",
    "\n",
    "<small>\n",
    "    \n",
    "Подробнее:\n",
    "\n",
    "Введём обозначения: \n",
    "$$\\begin{align}\n",
    "\\mathcal D &= \\{(x_1, y_1), \\ldots, (x_n, y_n)\\},\\\\\n",
    "L(t)&=\\{(x_i, y_i) \\in \\mathcal D\\mid x_i < t\\},\\\\\n",
    "R(t)&=\\{(x_i, y_i) \\in \\mathcal D\\mid x_i \\ge t\\},\\\\\n",
    "\\pi(y, M)&=\\frac{|\\{(x_i, y_i) \\in M\\mid y_i = y\\}|}{|M|},\n",
    "\\end{align}\n",
    "$$\n",
    "то есть $\\mathcal D$ — обучающая выборка, $L(t)$ и $R(t)$ — множества, на которые она разбивается по пороговому значению $t$, то есть $\\pi(y, M)$ — это доля элементов $M$, имеющих данный класс $y$. Тогда $I_G$ определяется следующим образом:\n",
    "\n",
    "$$I_G(M)=\\sum_{y\\in \\{good,\\, bad\\}} \\pi(y, M)\\cdot(1-\\pi(y, M)).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 4\n",
    "Профессор Буст считает, что профессор Стамп из предыдущей задачи выбрал неудачный алгоритм для предсказания погоды и вместо случайного леса лучше использовать градиентный бустинг. В качестве стартового базового алгоритма $a_0(x)$ бустинга он выбрал такой, который предсказывает, что вероятность наступления хорошей погоды равна одной второй, вне зависимости от значения x, то есть $a_0(x)=\\frac{1}{2}$ для всех $x$. В качестве функции потерь $L(y, p)$ он выбрал log loss (см. задачу 1). Общие потери на всей выборке вычисляются как сумма потерь на каждом объекте:\n",
    "\n",
    "$$\\mathcal L(p_1, \\ldots, p_n)=\\sum_{i=1}^n L(p_i, y_i)$$\n",
    "\n",
    "1. Рассмотрим выборку из предыдущей задачи. Найти вектор предсказаний $p^0$ базового алгоритма $a_0$ на элементах выборки, то есть $p^0=(a_0(x_1), a_0(x_2), a_0(x_3), a_0(x_4))$. \n",
    "2. Найти антиградиент $(-\\nabla \\mathcal L)$ по $p$ в точке $p=p^0$. Обозначим полученный вектор через $s=(s_1, s_2, s_3, s_4)$.\n",
    "3. Пусть $b_1(x)$ — решающий пень, обученный на выборке, у которой значения $x_i$ совпадают со значениями в исходной выборке, а значения целевой переменной равны $s_i$, то есть пень должен предсказывать $s_i$ по $x_i$ (это задача регрессии). В качестве предсказаний пень будет использовать среднее значение целевой переменной среди всех объектов выборки, которые попали в соответствующую часть. Пороговое значение $t$ выбирается таким образом, чтобы минимизировать квадратичную ошибку:\n",
    "$$\\newcommand{\\RSS}{\\mathop{\\mathrm{RSS}}}Q(L, R)=\\RSS(L)+\\RSS(R),$$\n",
    "где \n",
    "$$\\begin{align}\n",
    "\\RSS(M)&=\\sum_{(x_i, s_i) \\in M} (s_i - \\bar s)^2;\\\\\n",
    "\\bar s&=\\frac{1}{|M|}\\sum_{(x_i, s_i)\\in M} s_i.\n",
    "\\end{align}$$\n",
    "Найти пороговое значение $t$ и выписать функцию $b_1(x)$ явно.\n",
    "4. Пусть $a_1(x)=a_0(x)+b_1(x)$ — результат одного шага градиентного бустинга (коэффициент при $b_1$ для простоты выбрали равным 1). Найти $a_1(7)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 5\n",
    "Доцент Брейн считает, что деревья — устаревший метод машинного обучения, и предлагает применить к задаче определения завтрашней погоды нейросеть, изображённую на рисунке.\n",
    "\n",
    "![](http://math-info.hse.ru/f/2018-19/math-ml/nn.png)\n",
    "\n",
    "У этой нейросети два скрытых слоя, один вход ($x$) и один выход ($p$). Функция активации на скрытых слоях — \n",
    "$\\newcommand{\\ReLU}{\\mathop{\\mathrm{ReLU}}}\\ReLU(x)=\\max(0, x)$, на выходном слое — сигмоида $\\sigma(z)=\\frac{e^z}{1+e^z}$. Значение на выходе интерпретируется как вероятность того, что $y=good$. В качестве функции потерь используется всё тот же log loss.\n",
    "\n",
    "Формально:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "h^{1}_i& = \\ReLU(w^{1}_{1i}x),&i=1, 2,\\\\\n",
    "h^{2}_i&=\\ReLU(w^2_{1i}h^1_1+w^2_{2i}h^1_2),&i=1, 2,\\\\\n",
    "p&=\\sigma(w^3_{11}h^2_1+w^3_{21}h^2_2).\n",
    "\\end{align}\n",
    "$$\n",
    "Брейн запустила обучение с помощью стохастического градиентного спуска. Исходно веса оказались инициализированы таким образом, как показано в таблице\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "w^1_{11}& w^1_{12} & w^{2}_{11} & w^2_{12} & w^2_{21} & w^2_{22} & w^3_{11} & w^3_{21}\\\\\n",
    "\\hline\n",
    "1 & {-1} & 1 & 2 & -2 & -1 & 3 & -2\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Обозначим совокупность всех весов через $W$, значение $p$ для входа $x$ и весов $W$ через $p=f(x, W)$.\n",
    "\n",
    "1. Найти предсказание $p$ нейросети для $x=3$.\n",
    "2. Найти производную функции потерь $L(y, f(x, W))$ по $w^1_{11}$ для третьего наблюдения выборки ($x=3$, $y=bad$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 6\n",
    "Старший преподаватель Конвалёв считает, что для эффективного предсказания погоды недостаточно знать, сколько раз кукушка прокуковала перед закатом. По его мнению, нужны данные за несколько предыдущих дней — хотя бы за три. Для их анализа он применяет свёрточную нейросеть. Свёрточный слой получает на вход вектор из $\\mathbb R^3$ (число кукуков перед закатом в каждый из трёх предшествующих вечеров), применяет к нему одномерную свёртку с ядром размера 2 и на выход даёт двумерный вектор. Затем к этому вектору применяется max pool, то есть — в данном случае — просто вычисляется максимум из двух компонент этого вектора. К max pool применяется сигмоида, её значение интерпетируется как вероятность наступления хорошей погоды.\n",
    "\n",
    "Формально, пусть свёрточное ядро задаётся двумя весами $(w_1, w_2)$, на вход подаётся вектор $x=(x^1, x^2, x^3)$. Значения на выходе свёрточного слоя определяются следующим образом:\n",
    "\n",
    "$$h_i=\\sum_{j=1}^{2} x^{i+j-1}w_j,\\quad i=1, 2.$$\n",
    "\n",
    "Значение на выходе max pool определяется как $\\max(h_1, h_2)$. Выход всей сети определяется как $p=\\sigma(\\max(h_1, h_2))$.\n",
    "\n",
    "Обозначим выход нейросети для входного вектора $x$ и вектора весов $w$ через $p=f(x, w)$.\n",
    "\n",
    "При запуске стохастического градиентного спуска веса инициализировались значениями $w_1=2$, $w_2=-1$.\n",
    "\n",
    "1. Найти предсказание $p$ нейросети для $x=(2, 1, 3)$.\n",
    "2. Найти градиент функции потерь $L(y, f(x, w))$ по $w$ для $x=(2, 1, 3)$, $y=good$.\n",
    "3. Найти новые веса после одного шага стохастического градиентного спуска, считая, что коэффициент перед антиградиентом в градиентном спуске установлен в 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача 7\n",
    "Пусть пространство признаков двумерное (признаки $x^{(1)}$ и $x^{(2)}$), целевая переменная отсутствует. Выборка состоит из четырёх наблюдений: $(12, 9)$, $(-12, -9)$, $(-3, 4)$, $(3, -4)$.\n",
    "\n",
    "1. Постройте scatter plot для этой выборки.\n",
    "2. Найдите матрицу ковариации между признаками\n",
    "3. Найдите собственные векторы матрицы $X^T X$, где $X$ — матрица объект-признак для данной выборки. Нарисуйте их на картинке. Проверьте, что они ортогональны. Выберите их так, чтобы они имели единичную длину, причём первым вектором шёл тот, который соответствует большему собственному значению.\n",
    "4. Перейдите в базис, образованный собственными векторами. Координаты векторов в новом базисе образуют новые признаки. Как они связаны со старыми? Найдите значения новых признаков для всех наблюдений. Постройте scatter plot в новых координатах.\n",
    "5. Найдите матрицу ковариации между новыми признаками.\n",
    "\n",
    "Найденные вами признаки называются первой и второй главными компонентами, обозначаются $PC_1$, $PC_2$.\n",
    "### Задача 8\n",
    "\n",
    "Пусть пространство признаков двумерное, целевая переменная отсутствует и есть $n$ наблюдений $x_i\\in \\mathbb R^2$, $i=1, \\ldots, n$, признаки центрированы (то есть имеют нулевые средние). Пусть $u \\in \\mathbb R^2$ — некоторый вектор с единичной длиной. Для каждого наблюдения $x_i\\in \\mathbb R^2$, найдём проекцию $x_i$ на $u$. Обозначим её длину (с учётом знака) через $\\newcommand{\\proj}{\\mathrm{\\mathop{proj}}}\\proj_u(x_i)$. \n",
    "\n",
    "1. Докажите, что исправленная выборочная дисперсия для набора чисел $\\{\\proj_u(x_i)\\mid i=1,\\ldots, n\\}$ равна $\\frac{1}{n-1}u^T X^T Xu$.\n",
    "2. Найдите направление, для которого эта дисперсия максимальна. (Подсказка: это собственный вектор матрицы $X^T X$. Какой из них и почему?)\n",
    "3. Найдите направление, для которого эта дисперсия минимальна."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
