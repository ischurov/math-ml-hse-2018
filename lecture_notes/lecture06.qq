\chapter Ещё о линейной регрессии \label chap:6:linear-reg2

\section Напоминание: постановка задачи и метод наименьших квадратов

Наша модель выглядит следующим образом. Есть набор $x_1, \ldots, x_n \in \mathbb
R^d$. Мы их считаем фиксированными (неслучайными). Также есть некоторый
фиксированный вектор весов $w\in \mathbb R^d$. Значения $y_i$ определяются
следующим образом:

\equation \label eq:6:y_of_x
    y_i = \langle w, x_i \rangle + \eps_i,
где $\eps_i$ — случайные величины со следующими свойствами:

\enumerate
    \item $\mathbb E[\eps_i]=0$;
    \item $\mathbb D[\eps_i] = \sigma^2 < \infty$;
    \item $\newcommand{\cov}{\mathrm{\mathop{cov}}}\cov(\eps_i, \eps_j)=0$ для $i\ne j$.
Напомним, что \emph{ковариацией} случайных величин $\xi$ и $\eta$ называется число  
\eq
    \cov (\xi, \eta) = \mathbb E[(\xi - \mathbb E[\xi])(\eta - \mathbb
    E[\eta])].
Если случайные величины независимы, их ковариация равна нулю.

Обозначим через $X$ матрицу, в которой по строкам записаны векторы $x_i$. Это
матрица с $n$ строками и $d$ столбцами. Пусть $y=(y_1, \ldots, y_n)$ — вектор
правильных ответов и $\eps = (\eps_1, \ldots, \eps_n)$ — вектор ошибок.
Уравнение \ref{eq:6:y_of_x} можно записать в матричной форме:

\eq
    y = Xw + \eps

МНК-оценкой для $w$ называется  вектор $\hat w \in \mathbb R^d$,
который является решением оптимизационной задачи
\equation \label eq:6:OLS-prob
    \\|X\hat w - y \\|^2 \to \min_{\hat w}.
Если столбцы матрицы $X$ линейно независимы, то решение этой задачи является
единственным и его можно найти с помощью формулы
\equation \label eq:6:hat-w
    \hat w = (X^T X)^{-1} X^T y.

\remark
    В предыдущей лекции мы нашли МНК-оценку как оценку наибольшего правдоподобия
    в предположении, что остатки $\eps_i$ распределены по нормальному закону.
    Сейчас мы не делаем такого предположения, однако продолжаем рассматривать
    МНК-оценку. Оказывается, она обладает множеством хороших свойств даже в том
    случае, когда остатки не являются нормальными.

\subsection Геометрическая интерпретация

Рассмотрим пространство $\mathbb R^n$. Обозначим столбцы матрицы $X$ через
$x^{(1)}, \ldots, x^{(d)}$, они являются элементами $\mathbb R^n$. Рассмотрим
множество $L=\\{Xw \mid w \in \mathbb R^n \\} $. Оно задаёт $d$-мерное линейное
подпространство в пространстве $\mathbb R^n$, натянутое на столбцы матрицы $X$.
Оптимизационная задача \ref{eq:6:OLS-prob} состоит в нахождении точки $\hat y =
X \hat w$, ближайшей к точке $y$. Чтобы найти такую точку, достаточно
спроектировать $y$ на $L$ ортогонально. Чтобы найти $\hat w$, нужно разложить
$\hat y$ по базису в $L$, составленному из столбцов матрицы $X$.

Эта интерпретация часто бывает полезна, но про некоторые вещи с её помощью
невозможно думать: например, невозможно себе представить, что значит «найти
предсказание для нового $x$ (отличного от тех, что есть в обучающей выборке)». 

\section Несмещённость МНК-оценки
\proposition
    МНК-оценка является несмещённой, то есть
    \eq
        \mathbb E[\hat w]=w.
\proof
    Для доказательства нам потребуются две вспомогательные леммы (очень простые).
    \lemma \label lem:6:E-prod
        Матожидание коммутирует со скалярным произведением на фиксированный
        вектор, то есть для любого случайного вектора $z \in \mathbb R^k$ и
        постоянного вектора $u \in \mathbb R^k$
        \eq
            \mathbb E[\langle u, z \rangle]=\langle u, \mathbb E[z]\rangle
    \proof \of леммы
        Это переформулировка линейности матожидания. Пусть $u = (u_1, \ldots,
        u_k)$ и $z=(z_1, \ldots, z_k)$. Тогда
        \eq
            \mathbb E[\langle u, z \rangle]=\mathbb E[u_1 z_1 + \cdots + u_k z_k]=
            u_1 \mathbb E[z_1]+\cdots +u_k \mathbb E[z_k]=\langle u, \mathbb
            E[z]\rangle.
    \lemma \label lem:6:E-A
        Матожидание коммутирует с умножением на фиксированную матрицу, то есть
        для любого случайного вектора $z \in \mathbb R^k$ и постоянной матрицы
        $A$ с $k$ столбцами справедливо:
        \eq
            \mathbb E[Az]=A \mathbb E[z]
    \proof \of леммы
        Каждая компонента вектора $Az$ является скалярным произведением строки
        матрицы $A$ на вектор $z$. Дальше применяем \ref[лемму][lem:6:E-prod]
        покомпонентно и получаем требуемое.

    \ 
    Вернёмся к доказательству утверждения. Имеем:
    \align\nonumber
        \item
            \mathbb E[\hat w]=&\mathbb E[(X^T X)^{-1} X^T y]
            =\mathbb E[(X^T X)^{-1} X^T (Xw+\eps)]=
        \item
            =&\mathbb E[(X^T X)^{-1} (X^T X) w] + \mathbb E[(X^T X)^{-1}X^T \eps]=
        \item
            =&\mathbb E[w] + (X^T X)^{-1}X^T \mathbb E[\eps]=\mathbb E[w].
    В предпоследнем переходе мы воспользовались \ref[леммой][lem:6:E-A], в
    последнем — предположением о том, что матожидание $\eps$ равно нулю.

\section Дисперсии и ковариации МНК-оценки

Из \ref[лекции][chap:4:bias-variance] мы помним, что на ожидаемую ошибку для
новых наблюдений влияют три фактора: шум в данных, систематическая ошибка
предсказания (смещение) и разброс предсказания. Первый фактор мы никак не
контролируем. Только что мы показали, что метод наименьших квадратов даёт
несмещённую оценку для коэффициентов, и следовательно (по
\ref[лемме][lem:6:E-prod]), несмещенную оценку для предсказания. (Напомним, что
предсказание для наблюдения с заданным вектором $x$ есть скалярное произведение
$\langle x, \hat w \rangle$.) Что с разбросом, то есть дисперсией предсказаний?

\subsection Ковариационная матрица
Чтобы сказать что-то про разброс предсказаний необходимо сначала разобраться с
разбросом вектора весов $\hat w$. У числовой случайной величины есть дисперсия.
Разброс векторной случайной величины характеризует более сложный объект
— ковариационная матрица.

\definition
    \emph{Ковариационной матрицей} векторной случайной величины $z$, принимающей
    значения в $\mathbb R^k$, называется матрица, имеющая вид:
    \eq
        \newcommand{\Var}{\mathrm{\mathop{Var}}}
        \Var (z) = 
            \begin{pmatrix}
                \cov(z_1, z_1)& \cov(z_1, z_2)& \ldots& \cov(z_1, z_k)\\\\
                \cov(z_2, z_1)& \cov(z_2, z_2) & \ldots & \cov(z_2, z_k)\\\\
                \vdots & \vdots & \ddots & \vdots \\\\
                \cov(z_k, z_1) & \cov(z_k, z_2) & \ldots & \cov(z_k, z_k)
            \end{pmatrix}.
    Эта матрица является симметричной. Поскольку ковариация случайной величины с
    самой собой является её дисперсией, на диагонали ковариационной матрицы
    стоят как раз дисперсии компонент $z$.

Гм-гм, симметричная матрица? Наверняка она задаёт какую-нибудь симметричную
билинейную или квадратичную форму! И правда.

\proposition
    Ковариационная матрица задаёт билинейную форму. Для любых фиксированных
    векторов $u, v \in \mathbb R^k$:
    \eq
        \langle \Var(z) u, v \rangle = u^T \Var(z) v = \mathbb E[\langle u, z -
        \mathbb E[z]\rangle \langle v, z - \mathbb E [z]\rangle].
\proof
    В силу линейности, достаточно проверить это утверждение, выбирая в качестве
    $u$ и $v$ базисные векторы. Для таких векторов оно проверяется методом
    пристального вглядывания в формулы.

\corollary \label cor:6:cov
    Ковариационная матрица задаёт квадратичную форму, значение которой на
    векторе $u$ равно дисперсии скалярного произведения $\langle u, z\rangle$:
    \eq
        \langle \Var(z) u, u \rangle = u^T \Var(z) u = \mathbb D[\langle u,
        z\rangle].

\subsection Пример и геометрическая интерпретация
На \ref[рис.][fig:6:two-gaussian] изображены выборки из двух двумерных
нормальных распределений, отличающихся ковариационной матрицей. 

\figure \label fig:6:two-gaussian
    \pythonfigure
        z = np.random.multivariate_normal(np.array([0, 0]), 
                                          np.array([[1, 0], 
                                                    [0, 1]]), 500)
        w = np.random.multivariate_normal(np.array([0, 0]), 
                                          np.array([[1, 0.8], 
                                                    [0.8, 1]]), 500)
        for i, v in enumerate([z, w], 1):
            ax = plt.subplot(120 + i)
            ax.set_aspect('equal')
            plt.xlim(-3, 3)
            plt.ylim(-3, 3)
            plt.plot(v[:, 0], v[:, 1], 'o', markersize=3, alpha=0.5)

    \caption Две выборки из двумерного нормального распределения с различными
            матрицами ковариации

Для левой
картинки ковариационная матрица является единичной:
\eq
    \begin{pmatrix}
        1 & 0 \\\\
        0 & 1
    \end{pmatrix}.
Поскольку на внедиагональном элементе матрицы стоит 0, компоненты вектора
оказываются нескоррелированными, что мы и видим по картинке: увеличение
горизнтальной компоненты не приводит к систематическому увеличению или
уменьшению вертикальной, линейная зависимость отсутствует.

Для правой картинки матрица ковариации равна
\eq
    \begin{pmatrix}
        1 & 0{,}8 \\\\
        0{,}8 & 1
    \end{pmatrix}.
Каждая из компонент по отдельности имеет такой же разброс, как и на предыдущей
картинке, но зато теперь эти две компоненты не являются независимыми. Ковариация
между ними равна $0{,}8$ и на картинке мы видим явную зависимость между
горизонтальной и вертикальной компонентами случайной величины.

\ref[Следствие][cor:6:cov] имеет следующую геометрическую интерпретацию.
Зафиксируем какой-нибудь вектор $u$, имеющий единичную длину. Тогда $u^T \Var(z)
u$ показывает, какова дисперсия скалярного произведения $\langle u, z\rangle$.
Для единичного вектора $u$ указанное скалярное произведение — это длина проекции
$z$ на $u$. То есть мы измеряем разброс проекции случайного вектора на заданное
направление. Например, на правой картинке колебания вдоль направления вектора
$(\sqrt{2}/2, \sqrt{2}/2)$ будет гораздо больше, чем вдоль направления вектора
$(-\sqrt{2}/2, \sqrt{2}/2)$. На левой картинке колебания в любом направлении одинаковы.

\subsection Ковариационная матрица и линейные операторы
\proposition \label prop:6:Var-A
    Для случайного вектора $z$ со значениями в $\mathbb R^k$ и фиксированной
    матрицы $A$ с $k$ столбцами справедливо следующее:
    \eq
        \Var(Az)=A \Var(z) A^T.
\proof
    Возьмём произвольный фиксированный вектор $u \in \mathbb R^k$. Имеем:
    \eq
        u^T \Var(Az) u = \mathbb D[\langle u, Az \rangle] = \mathbb D[\langle z,
        A^T u \rangle] = (A^T u)^T \Var(z) A^T u = u^T(A \Var(z) A^T)u.
    Поскольку равенство выполняется для любого вектора $u$, матрицы между $u^T$
    и $u$ в левой и правой части цепочки равенств обязаны совпадать.

\subsection Ковариационная матрица МНК-оценки

Теперь всё готово к тому, чтобы найти ковариационную матрицу для оценки вектора
весов $\hat w$. Подставим в формулу \ref{eq:6:hat-w} модель
\ref{eq:6:y_of_x} и посчитаем ковариационную матрицу:
\eq
    \Var (\hat w) = \Var((X^T X)^{-1} X^T (Xw + \eps)) = \Var(w + (X^T X)^{-1}
    X^T \eps)=\ldots
Сдвиг на постоянный вектор $w$ не влияет на ковариации (из каждой компоненты всё
равно вычитается её матожидание).
\eq
    \ldots = \Var((X^T X)^{-1} X^T \eps)=(X^T X)^{-1} X^T \Var(\eps)((X^T X)^{-1} X^T)^T=\ldots
Здесь мы воспользовались \ref[утверждением][prop:6:Var-A]. Ковариационная
матрица $\eps$ является скалярной (по предположениям модели — все дисперсии
равны одному и тому же числу, все ковариации равны нулю) и равна
$\sigma^2 I$.
\eq
    \ldots = \sigma^2 (X^T X)^{-1} X^T X (X^T X)^{-1}=\sigma^2 (X^T X)^{-1}.
Ура! Итак,
\equation \label eq:6:var-hat-w
    \Var(\hat w)=\sigma^2 (X^T X)^{-1}.
Таким образом, зная матрицу $X$ мы можем явно найти ковариационную матрицу для
$\hat w$ (по крайней мере, если верим в предположения нашей модели).

\subsection Теорема Гаусса — Маркова
Хороша ли МНК-оценка? Теорема Гаусса — Маркова говорит: ой как хороша! По
крайней мере, если сравнивать её с другими несмещенными линейными оценками.

\theorem (Гаусса — Маркова)
    Пусть выполняются условия модели \ref{eq:6:y_of_x}, матрица $X$ имеет
    независимые столбцы и пусть $\bar w$ — какая-то несмещённая линейная оценка
    для $w$, то есть $\bar w$ записывается в виде
    \eq
        \bar w = Cy,
    где $C$ — некоторая фиксированная матрица и
    \eq
        \mathbb E[\bar w]=w.
    Рассмотрим произвольный фиксированный вектор $x \in \mathbb R^d$. Тогда
    \eq
        \mathbb D[\langle x, \bar w \rangle] \ge \mathbb D[\langle x, \hat w
        \rangle],
    где $\hat w$, как и прежде, МНК-оценка для $w$.

Иными словами, теорема Гаусса — Маркова говорит, что дисперсия (разброс) любого
предсказания для любой линейной несмещённой оценки $w$ будет не меньше, чем
дисперсия того же предсказания для МНК-оценки.

Заключение теоремы можно также переформулировать таким образом: матрица
\eq
    \Var(\bar w)-\Var(\hat w)
всегда неотрицательно определена.

Доказывать эту теорему мы сейчас не будем.

\subsection Когда смещённая оценка лучше

Казалось бы, мы победили: нашли несмещённую оценку, которая даёт предсказания с
минимальной дисперсией. Чего ещё можно хотеть?

Теорема Гаусса — Маркова рассматривает только довольно узкий класс альтернатив
— исключительно линейные несмещённые оценки, и показывает, что МНК-оценка
оптимальна именно в этом классе. Но это не означает, что она оптимальна с
практической точки зрения.

Напомним (второй раз за сегодня), что ожидаемая ошибка на новом наблюдении (то,
что мы хотим сделать как можно менше) складывается из шума, смещения и разброса.
Мы показали, что МНК-оценка имеет нулевое смещение и минимальный разброс среди
оценок с нулевым смещением. Однако, может быть, есть оценка с ненулевым
смещением, которая имеет существенно более низкий разброс, и таким образом по
сумме выигрывает у МНК-оценки? Оказывается, что так как раз часто и бывает
(более того, почти всегда).

Давайте покажем, как это возможно, на простом примере.

\example
    Пусть матрица $X$ задана следующим образом:
    \eq
        X = 
        \begin{pmatrix}
            10 & 0  \\\\
            -10 & 0 \\\\
            0 & 1   \\\\
            0 & -1  \\\\
        \end{pmatrix}.
    Иными словами, у нас есть всего четыре наблюдения: в точках $(10, 0)$,
    $(-10, 0)$, $(0, 1)$, $(0, -1)$.

    Пусть также истинный вектор весов $w=(1, 1)$, то есть истинная зависимость
    имеет вид:
    \eq
        y_i = x_{i}^{(1)} + x_i^{(2)} + \eps_i

    Ковариационная матрица МНК-оценки имеет вид:
    \eq
        \Var(\hat w) = \sigma^2 (X^T X)^{-1}=
            \begin{pmatrix}
                \frac{\sigma^2}{200} & 0 \\\\
                0 & \frac{\sigma^2}{2}
            \end{pmatrix}.

    Предсказания в точках $(10, 0)$ и $(-10, 0)$ равны
    \eq
        \langle (\pm 10, 0), w \rangle = \pm 10 w_1.
    Их разброс равен
    \eq
        \mathbb D[\pm 10 w_1] = 10^2 \mathbb D[w_1] = 10^2
        \frac{\sigma^2}{200}=\frac{\sigma^2}{2}.
    Аналогично разброс предсказаний в точках $(\pm 1, 0)$ также равен
    $\frac{\sigma^2}{2}$.

    Как мы видим, если $\sigma^2$ очень большое, разброс предсказаний МНК-модели
    может быть также очень большим.

    Давайте вместе с МНК-оценкой для исходной модели рассмотрим также МНК-оцеки
    для упрощённых моделей, которые игнорируют один из или оба признака. Иными
    словами, мы рассматриваем четыре модели. 
    \enumerate
        \item Наша исходная модель.

        \item Модель построена по матрице $X$, из которой убрали первый столбец
            и оставили только второй, зависимость предсказания от первого
            признака в модели отсутствует. Иными словами, в формуле
            \eq
                \hat y = \hat w_1 x^{(1)} + \hat w_2 x^{(2)}
            принудительно положили, что $\hat w_1=0$.

        \item Модель построена по матрице $X$, из которой убрали второй столбец и
            оставили только первый. Иными словами, принудительно положили
            $\hat w_2=0$.
        \item Модель вообще игнорирует матрицу $X$ и во всех точках $x$ делает
            предсказание $\hat y=0$. Иными словами, $\hat w_1 = \hat w_2 = 0$.

    Нетрудно показать, что для нашей матрицы $X$ необнулённые веса в моделях 2 и
    3 совпадают с соответствующими весами исходной модели.

    Давайте посчитаем ожидаемую ошибку для всех четырёх моделей. Для этого нужно
    найти смещение и разброс для каждой модели.

    У исходной (первой) модели нулевой смещение предсказаний, а разброс в каждой
    из четырёх точек равен $\frac{\sigma^2}{2}$. Общий разброс равен
    $2\sigma^2$.

    У второй модели в точках $(0, \pm 1)$ смещение остаётся нулевым, а в точках
    $(\pm 10, 0)$ модель предсказывает значение $0$ (поскольку мы обнулили $\hat
    w_1$), в то время как правильное значение равно $\pm 10$. Значит, смещение
    (вернее, его квадрат, именно он входит в формулу для ожидаемой ошибки) в
    каждой из этих точек равно $(0 \mp 10)^2=100$, общее смещение модели $200$.
    Разброс в точках $(0, \pm 1)$ остаётся равен $\frac{\sigma^2}{2}$, а в
    точках $(\pm 10, 0)$ разброс равен нулю, поскольку в этих точках модель
    предсказывает постоянное число (0). Общий разброс равен $\sigma^2$.

    У третьей модели всё наоборот — в точках $(0, \pm 1)$ смещение равно по 1
    (там истинное значение равно 1, а предсказание будет 0), общее смещение
    равно 2. Общий разброс, как и второй модели, оказывается равен $\sigma^2$.

    Наконец, у четвертой модели смещение равно $202$, а разброс нулевой.

    Сведём наши результаты в табличку.
    \eq
        \begin{array}{c|c|c|c}
            model & bias & variance \\\\
            \hline
            1 & 0 & 4\sigma^2 \\\\
            2 & 200 & 2\sigma^2 \\\\
            3 & 2 & 2\sigma^2 \\\\
            4 & 202 & 0 \\\\
            \hline
        \end{array}

    Если $\sigma^2$, то есть шум в наших данных, маленький (например, равен 0),
    то самой лучшей моделью будет первая. Однако, если $\sigma^2 > 1$,
    оказывается выгодно использовать вместо первой модели третью (которая
    игнорирует второй признак вообще). Конечно, наша модель окажется смещённой,
    но зато она будет выдавать не такие «шумные» предсказания. Наконец, если
    дальше увеличивать $\sigma^2$, в какой-то момент будет выгоднее вообще
    перейти на четвертую модель, которая верит в то, что $y$ всегда равен нулю.
    (При каком $\sigma^2$ это наступит?)

    Итак, на нашем примере мы видим, что бывают ситуации, когда лучше выбрать
    смещённую модель, которая даёт меньший разброс предсказаний, чем несмещённую
    модель. Это ещё один пример так называемого bias-variance tradeoff.

    Заметим, что в данном случае оптимальной могла стать третья модель, но никак
    не вторая: её ожидаемая ошибка при любом $\sigma^2$ больше ожидаемой ошибки
    третьей. Это можно интерпретировать так. В нашей истинной зависимости
    коэффициенты при обоих признаках были равны между собой. В то же время
    дисперсии самих признаков существенно различались — дисперсия первого
    признака была гораздо больше дисперсии второго. При равных дисперсиях шумов
    в каждой точке, это привело к тому, что дисперсия второй компоненты вектора
    признаков оказалась гораздо выше дисперсии первой. Поэтому именно ей нам
    пришлось «пожертвовать», чтобы уменьшить разброс предсказаний. На этой идее
    основан один из методов отбора признаков — удаление \emph{незначимых}
    признаков, то есть таких, у которых слишком большое значение разброса по
    сравнению со значением самого признака.

    Если предполагать, что веса в истинной зависимости примерно одинаковые и
    остальные предположения выполняются, большую дисперсию будут иметь веса,
    соответствующие признакам, которые сами имеют маленькую дисперсию (как
    второй признак в нашем примере). Это ещё один механизм отбора признаков.

    На семинаре мы также обсудим \emph{регуляризацию} — ещё один механизм
    уменьшения разброса в предсказаниях, который автоматически уменьшает веса,
    соответствующие признакам с маленькой дисперсией. 

    Заметим также, что проблемы, связанные со слишком большим разбросом
    предсказаний могут возникать не только в том случае, когда какой-то из
    признаков имеет маленькую дисперсию, но и когда какие-то признаки слишком
    сильно скоррелированы друг с другом. Механизмы, которые здесь работают,
    полностью аналогичны разобранным в нашем примере. Регуляризация позволяет
    справиться и с этой проблемой тоже.
